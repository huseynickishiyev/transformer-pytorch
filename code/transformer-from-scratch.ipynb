{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8474ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import math, copy, re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf41aa4",
   "metadata": {},
   "source": [
    "**Creating Embedding Vectors**\n",
    "\n",
    "- Each embedding vector's size is 512. If the vocab size is 100, that makes the embedding matrix 100x512. Similarly, if batch_size=64, the output will be 64x10x512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ffa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectors(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dimension):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            vocab_size: vocabulary size\n",
    "            embedding_dimension: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(EmbeddingVectors, self).__init__()\n",
    "        self.embed = nn.EmbeddingVectors(vocab_size, embedding_dimension)\n",
    "    \n",
    "    def fforward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            output: embedding vector\n",
    "        \"\"\"\n",
    "        output = self.embed(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b4c2e",
   "metadata": {},
   "source": [
    "**Positional Encoding**\n",
    "- In the paper, authors use sin() and cos() to create positional encoding. According to the paper, cosine is used for odd time steps and sine used in even time steps.\n",
    "- PE = sin(pos/10000^2i/d)\n",
    "- PE = cos(pos/10000^2i/d)\n",
    "- Where pos is order in the sentence and i is position. If we have a batch size of 32 and seq length of 10 and let embedding dimension be 512, positional encoding vector's dimension would be 32x10x512. Then we add embedding vector's dimension with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec1fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embed_model_dimension):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_model_dimension\n",
    "        \n",
    "        positional_encoding = torch.zeros(max_seq_len, self.embed_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.embed_dim, 2):\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.embed_dim)))\n",
    "                positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i)) / self.embed_dim)))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('pe', positional_encoding)\n",
    "        \n",
    "    def fforward(self, x):\n",
    "        x = x * np.sqrt(self.embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.positional_encoding[:, :seq_len], requires_grad=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946c12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension=512, n_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.n_heads = n_heads\n",
    "        self.single_head_dim = int(self.embedding_dimension / self.n_heads) #512 / 8 = 6 each key, query, value will be of 64d\n",
    "        \n",
    "        #key, query, value matrices 64 x 64\n",
    "        self.query_matrix = nn.Linear(self.single_head_dim, self.single_head_dim, bias=False) \n",
    "        self.key_matrix = nn.Linear(self.single_head_dim, self.single_head_dim, bias=False)\n",
    "        self.value_matrix = nn.Linear(self.n_heads * self.single_head_dim, self.embed_dim)\n",
    "        self.out = nn.Linear(self.n_heads * self.single_head_dim, self.embed_dim)\n",
    "        \n",
    "    def fforward(self, key, query, value, mask=None):\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        \n",
    "        seq_length_query = query.size(1)\n",
    "        \n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
    "        \n",
    "        key_ = self.key_matrix(key)\n",
    "        query_ = self.query_matrix(query)\n",
    "        value_ = self.value_matrix(value)\n",
    "        \n",
    "        query_ = query_.transpose(1,2)\n",
    "        key_ = key.transpose(1,2)\n",
    "        value_ = key.transpose(1,2)\n",
    "        \n",
    "        #computing attention\n",
    "        key_adjusted = key_.transpose(-1,2)\n",
    "        product = torch.matmul(query_, key_adjusted) #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10)\n",
    "        \n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            \n",
    "        product = product / math.sqrt(self.single_head_dim) #/sqrt(64)\n",
    "        \n",
    "        #softmax\n",
    "        scores = F.softmax(product, dim=-1)\n",
    "        \n",
    "        #multiply with value matrix\n",
    "        scores = torch.matmul(scores, v)\n",
    "        \n",
    "        #concatenate\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim * self.n_heads)\n",
    "        \n",
    "        output = self.out(concat) #(32, 10, 512) -> (32, 10, 512)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aacc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holden",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
