{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8474ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import math, copy, re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf41aa4",
   "metadata": {},
   "source": [
    "**Creating Embedding Vectors**\n",
    "\n",
    "- Each embedding vector's size is 512. If the vocab size is 100, that makes the embedding matrix 100x512. Similarly, if batch_size=64, the output will be 64x10x512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ffa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dimension):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            vocab_size: vocabulary size\n",
    "            embedding_dimension: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            output: embedding vector\n",
    "        \"\"\"\n",
    "        output = self.embed(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b4c2e",
   "metadata": {},
   "source": [
    "**Positional Encoding**\n",
    "- In the paper, authors use sin() and cos() to create positional encoding. According to the paper, cosine is used for odd time steps and sine used in even time steps.\n",
    "- PE = sin(pos/10000^2i/d)\n",
    "- PE = cos(pos/10000^2i/d)\n",
    "- Where pos is order in the sentence and i is position. If we have a batch size of 32 and seq length of 10 and let embedding dimension be 512, positional encoding vector's dimension would be 32x10x512. Then we add embedding vector's dimension with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec1fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embed_model_dimension):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_model_dimension\n",
    "        \n",
    "        positional_encoding = torch.zeros(max_seq_len, self.embed_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.embed_dim, 2):\n",
    "                positional_encoding[pos, i] = torch.sin(pos / (10000 ** ((2 * i) / self.embed_dim)))\n",
    "                positional_encoding[pos, i + 1] = torch.cos(pos / (10000 ** ((2 * (i)) / self.embed_dim)))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('pe', positional_encoding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x * np.sqrt(self.embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:, :seq_len], requires_grad=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3d99d",
   "metadata": {},
   "source": [
    "**MultiHeadAttention**\n",
    "\n",
    "- Implementation of the multi-head attention mechanism which takes in -key-, -query-, -value- tensors each of shape (*batch_size*, *sequence_length*, *embedding_dimension*)\n",
    "\n",
    "- The embedding_dimension is then split into n_heads and the multi-head attention is performed separately. Individual attention scores are concatenated and passed through a linear layer.\n",
    "\n",
    "- To allow for parallel computation, *key*, *query* and *value* tensors are reshaped so that the *embedding_dimension* is split into n_heads equally. Thus, the *key* and *query* tensors are used to compute attention scores.\n",
    "\n",
    "- A mask could be passed in to mask out particular elements in the *key* tensor. It's a useful method when performing seq2seq tasks, where attention mechanism needs to attend only to the previous elements in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946c12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dimension=512, n_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.n_heads = n_heads\n",
    "        self.single_head_dim = int(self.embedding_dimension / self.n_heads) #512 / 8 = 6 each key, query, value will be of 64d\n",
    "        \n",
    "        #key, query, value matrices 64 x 64\n",
    "        self.query_matrix = nn.Linear(self.single_head_dim, self.single_head_dim, bias=False) \n",
    "        self.key_matrix = nn.Linear(self.single_head_dim, self.single_head_dim, bias=False)\n",
    "        self.value_matrix = nn.Linear(self.n_heads * self.single_head_dim, self.embedding_dimension)\n",
    "        self.out = nn.Linear(self.n_heads * self.single_head_dim, self.embed_dim)\n",
    "        \n",
    "    def fforward(self, key, query, value, mask=None):\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        \n",
    "        seq_length_query = query.size(1)\n",
    "        \n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
    "        \n",
    "        key_ = self.key_matrix(key)\n",
    "        query_ = self.query_matrix(query)\n",
    "        value_ = self.value_matrix(value)\n",
    "        \n",
    "        query_ = query_.transpose(1,2)\n",
    "        key_ = key.transpose(1,2)\n",
    "        value_ = value.transpose(1,2)\n",
    "        \n",
    "        #computing attention\n",
    "        key_adjusted = key_.transpose(-1,2)\n",
    "        product = torch.matmul(query_, key_adjusted) #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10)\n",
    "        \n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            \n",
    "        product = product / math.sqrt(self.single_head_dim) #/sqrt(64)\n",
    "        \n",
    "        #softmax\n",
    "        scores = F.softmax(product, dim=-1)\n",
    "        \n",
    "        #multiply with value matrix\n",
    "        scores = torch.matmul(scores, value_)\n",
    "        \n",
    "        #concatenate\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim * self.n_heads)\n",
    "        \n",
    "        output = self.out(concat) #(32, 10, 512) -> (32, 10, 512)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06aacc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embedding_dimension=512, n_heads=8, fforward_dimension=2048):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embedding_dimension, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dimension)\n",
    "        \n",
    "        self.fforward = nn.Sequential(\n",
    "            nn.Linear(embedding_dimension, fforward_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fforward_dimension, embedding_dimension),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dimension)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attention_output = self.attention(x, x, x, mask)\n",
    "        norm1_output = self.norm1(x + attention_output)\n",
    "        \n",
    "        fforward_output = self.fforward(norm1_output)\n",
    "        norm2_output = self.norm2(norm1_ouput + fforward_output)\n",
    "        \n",
    "        return norm2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410ff64",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa9f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dimension, n_heads, fforward_dimension, dropout_rate):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([TransformerLayer(embedding_dimension, n_heads, fforward_dimension, dorpout_rate) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dimension)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holden",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
